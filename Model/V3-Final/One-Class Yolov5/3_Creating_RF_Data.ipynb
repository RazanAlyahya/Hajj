{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735d61db",
   "metadata": {},
   "source": [
    "<h1> Eyes of Hajj </h1>\n",
    "<h3>Hajj Crowd Abnormal Behavior Detection</h3>\n",
    "<h3>Project Workflow</h3>\n",
    "<ol>\n",
    "    <li>Loading the HAJJv2 Dataset.</li>\n",
    "    <li>Extracting the images (frames) from the videos and merge the labesls into a single file for train & test.</li>\n",
    "    <li>Transforming the data to Yolo format where we optain a text file (label) for each image in train & test & val. </li>\n",
    "    <li>Changeing the Yolo feature extractor's backbone to MobileNet/ResNet50 to optimize the feature extraction.</li>\n",
    "    <li>Applaying the Lucas-Kanade algorithm on the frames (images) from the original dataset to estimate the optical flow (Magnitude & Orientation/Direction).</li>\n",
    "    <li>Taking the mean & varience & STD of the optical flow and feed it into the Random Forest classifier to predect the actual class of the abnormal behavior.</li>\n",
    "    <li>Using the Kalman filter with the RF predections to optimaize the Yolo's object tracking.</li>\n",
    "    <li>Evaluating the models' performences on the testing data nad save the output in a new folder.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c0fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from cv2 import goodFeaturesToTrack\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581f271",
   "metadata": {},
   "source": [
    "### 4. Changeing the Yolo feature extractor's backbone to MobileNet/ResNet50 to optimize the feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5754493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\oalya/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-9-20 Python-3.11.3 torch-2.0.1+cpu CPU\n",
      "\n",
      "Loading best.onnx for ONNX Runtime inference...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx', 'onnxruntime'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache \"onnx\" \"onnxruntime\" ' returned non-zero exit status 1.\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Loading the best Yolo weights in ONNX format\n",
    "model_path = './best.onnx'\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613bc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Abnormal Objects with YOLOv5 (Batch Processing)\n",
    "def detect_abnormal_objects(frames, yolov5_model, confidence_threshold=0.5):\n",
    "    abnormal_objects = []\n",
    "\n",
    "    for frame in tqdm(frames, desc=\"Detecting Abnormal Objects\"):\n",
    "        # Convert frame to BGR format (YOLOv5 expects BGR)\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Perform object detection using YOLOv5\n",
    "        results = yolov5_model(frame_bgr)\n",
    "\n",
    "        # Extract bounding box data from YOLOv5 results\n",
    "        bboxes = results.pandas().xyxy[0]\n",
    "\n",
    "        # Filter bounding boxes based on confidence threshold\n",
    "        filtered_bboxes = bboxes[bboxes['confidence'] >= confidence_threshold]\n",
    "\n",
    "        # Convert the filtered bounding boxes to a list of tuples\n",
    "        abnormal_bboxes = [(bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]) for bbox in filtered_bboxes[['xmin', 'ymin', 'xmax', 'ymax', 'class']].values]\n",
    "\n",
    "        abnormal_objects.append(abnormal_bboxes)\n",
    "\n",
    "    return abnormal_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc126b",
   "metadata": {},
   "source": [
    "### 5. Applaying the Lucas-Kanade algorithm on the frames (images) from the original dataset to estimate the optical flow (Magnitude & Orientation/Direction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d012a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow_lucas_kanade(prev_frame, next_frame, prev_pts):\n",
    "    # Calculate optical flow using Lucas-Kanade method\n",
    "    next_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_frame, next_frame, prev_pts, None)\n",
    "\n",
    "    # Extract u and v components of optical flow\n",
    "    uv_flow = next_pts - prev_pts\n",
    "\n",
    "    return uv_flow, status\n",
    "\n",
    "\n",
    "def calculate_optical_flow(frames, abnormal_objects, video_no, frame_no, target_size):\n",
    "    optical_flow_frames = []\n",
    "\n",
    "    for frame, object_info in tqdm(zip(frames, abnormal_objects), desc=f\"Calculating Optical Flow for Video {video_no}\"):\n",
    "        optical_flow_frames_individual = []\n",
    "\n",
    "        # Detect key points in the frame\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        prev_pts = goodFeaturesToTrack(gray_frame, maxCorners=100, qualityLevel=0.3, minDistance=7)\n",
    "\n",
    "        for i in range(len(object_info) - 1):\n",
    "            bbox1 = object_info[i]\n",
    "            bbox2 = object_info[i+1]\n",
    "\n",
    "            # Unpack bounding box coordinates and convert them to integers\n",
    "            x1, y1, x2, y2, confidence1 = bbox1\n",
    "            x3, y3, x4, y4, confidence2 = bbox2\n",
    "\n",
    "            w1, h1 = int(x2 - x1), int(y2 - y1)\n",
    "            w2, h2 = int(x4 - x3), int(y4 - y3)\n",
    "\n",
    "            object1 = frame[int(y1):int(y1 + h1), int(x1):int(x1 + w1)]\n",
    "            object2 = frame[int(y3):int(y3 + h2), int(x3):int(x3 + w2)]\n",
    "\n",
    "            # Resize objects to a common size\n",
    "            object1 = cv2.resize(object1, target_size)\n",
    "            object2 = cv2.resize(object2, target_size)\n",
    "\n",
    "            # Calculate optical flow using Lucas-Kanade method with previously detected points\n",
    "            uv_flow, _ = compute_optical_flow_lucas_kanade(object1, object2, prev_pts)\n",
    "\n",
    "            optical_flow_frames_individual.append(uv_flow)  # Append the optical flow for this pair of objects\n",
    "\n",
    "        optical_flow_frames.append(optical_flow_frames_individual)\n",
    "\n",
    "    return optical_flow_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05466f02",
   "metadata": {},
   "source": [
    "### 6. Taking the mean & varience & STD of the optical flow and feed it into the Random Forest classifier to predect the actual class of the abnormal behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"./LK_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aee25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "features = ['Orientation Mean', 'Orientation Variance', 'Orientation Std Deviation',\n",
    "       'Magnitude Mean', 'Magnitude Variance', 'Magnitude Std Deviation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class_Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdaf4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optical_flow_features(optical_flow_frames):\n",
    "    features = []\n",
    "\n",
    "    for optical_flow_frames_individual in tqdm(optical_flow_frames, desc=\"Extracting Optical Flow Features\"):\n",
    "        orientations = []\n",
    "        magnitudes = []\n",
    "\n",
    "        for uv_flow in optical_flow_frames_individual:\n",
    "            u, v = np.split(uv_flow, 2, axis=-1)  # Split uv_flow into u and v components\n",
    "            orientations.append(np.arctan2(v, u))\n",
    "            magnitudes.append(np.sqrt(u**2 + v**2))\n",
    "\n",
    "        orientations_mean = np.mean(orientations)\n",
    "        orientations_var = np.var(orientations)\n",
    "        orientations_std = np.std(orientations)\n",
    "        magnitudes_mean = np.mean(magnitudes)\n",
    "        magnitudes_var = np.var(magnitudes)\n",
    "        magnitudes_std = np.std(magnitudes)\n",
    "\n",
    "        features.append([orientations_mean, orientations_var, orientations_std,\n",
    "                         magnitudes_mean, magnitudes_var, magnitudes_std])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Abnormality with Random Forest\n",
    "def predict_abnormality(features, trained_rf_classifier):\n",
    "    predictions = trained_rf_classifier.predict(features)\n",
    "    return predictions\n",
    "\n",
    "# Modify this function to save features to a CSV file\n",
    "def save_features_to_csv(features, csv_filename):\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write a header row if needed\n",
    "        csv_writer.writerow([\"Orientation Mean\", \"Orientation Variance\", \"Orientation Std Deviation\", \"Magnitude Mean\", \"Magnitude Variance\", \"Magnitude Std Deviation\"])\n",
    "        csv_writer.writerows(features)\n",
    "\n",
    "target_size=(640, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8af02",
   "metadata": {},
   "source": [
    "### 7. Using the Kalman filter with the RF predections to optimaize the Yolo's object tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a226dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display annotated frames with bounding boxes and predictions\n",
    "def display_annotated_frames(frames, annotations):\n",
    "    for frame, annotation in zip(frames, annotations):\n",
    "        frame_number = annotation['Frame_Number']\n",
    "        bboxes = annotation['Bboxes']\n",
    "        predictions = annotation['Predictions']\n",
    "\n",
    "        for bbox, prediction in zip(bboxes, predictions):\n",
    "            x1, y1, x2, y2, _ = bbox  # Extract coordinates and class\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Add prediction text\n",
    "            cv2.putText(frame, f'Prediction: {prediction}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the frame with annotations\n",
    "        cv2.imshow('Annotated Frame', frame)\n",
    "        cv2.waitKey(0)  # Wait for a key press to display the next frame\n",
    "\n",
    "# Main code for processing multiple videos\n",
    "def process_multiple_videos(video_directory, yolov5_model, rf_classifier, confidence_threshold=0.5):\n",
    "    # Create a dictionary to store video frames and frame numbers\n",
    "    video_frames = defaultdict(list)\n",
    "\n",
    "    # List all video files in the directory\n",
    "    video_files = [os.path.join(video_directory, filename) for filename in os.listdir(video_directory) if filename.endswith('.mp4')]\n",
    "\n",
    "    # Extract video number from the video's filename\n",
    "    for video_file in video_files:\n",
    "        filename = os.path.basename(video_file)\n",
    "        video_no = int(filename.split(\"_\")[0].split(\".\")[0])  # Extract the number before \".mp4\"\n",
    "        frame_no = 0  # Initialize frame number\n",
    "\n",
    "        # Capture video frames\n",
    "        video_capture = cv2.VideoCapture(video_file)\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Resize the frame to the target size (640x480)\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            video_frames[video_no].append((frame, frame_no))  # Store frame and frame number\n",
    "            frame_no += 1  # Increment frame number\n",
    "\n",
    "        video_capture.release()\n",
    "\n",
    "    # Process frames for each video separately\n",
    "    for video_no, frames_info in video_frames.items():\n",
    "        frames, frame_numbers = zip(*frames_info)  # Unzip frame info\n",
    "\n",
    "        # Detect abnormal objects using YOLOv5\n",
    "        abnormal_objects = detect_abnormal_objects(frames, yolov5_model, confidence_threshold)\n",
    "\n",
    "        # Pass the video number and frame numbers to the calculate_optical_flow function\n",
    "        optical_flow_frames = calculate_optical_flow(frames, abnormal_objects, video_no, frame_no, target_size)\n",
    "        features = extract_optical_flow_features(optical_flow_frames)\n",
    "\n",
    "        # Define the CSV filename for this video\n",
    "        csv_filename = f\"video_{video_no}_features.csv\"\n",
    "\n",
    "        # Save the features to a CSV file\n",
    "        save_features_to_csv(features, csv_filename)\n",
    "\n",
    "        # Replace NaN values with zeros in the features\n",
    "        features = np.nan_to_num(features)\n",
    "\n",
    "        # Predict abnormality using the trained RF classifier\n",
    "        predictions = predict_abnormality(features, rf_classifier)\n",
    "\n",
    "        # Combine frame numbers, bounding boxes, and RF predictions into a list of dictionaries\n",
    "        annotations = []\n",
    "        for frame_number, bboxes, prediction in zip(frame_numbers, abnormal_objects, predictions):\n",
    "            annotations.append({\n",
    "                'Frame_Number': frame_number,\n",
    "                'Bboxes': bboxes,\n",
    "                'Predictions': prediction\n",
    "            })\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(annotations)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        csv_filename = \"annotations.csv\"\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # Display annotated frames one by one\n",
    "        #display_annotated_frames(frames, abnormal_objects)\n",
    "\n",
    "    cv2.destroyAllWindows()  # Close the window after processing all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f85a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Abnormal Objects:   5%|▌         | 35/700 [02:35<16:33,  1.49s/it]  WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [17:25<00:00,  1.49s/it]\n",
      "Calculating Optical Flow for Video 10: 700it [00:16, 43.49it/s]\n",
      "Extracting Optical Flow Features:   0%|          | 0/700 [00:00<?, ?it/s]c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  .. versionadded:: 1.20.0\n",
      "c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rcount = um.maximum(rcount - ddof, 0)\n",
      "c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3715: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  :ref:`ufuncs-output-type`\n",
      "c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  )\n",
      "c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "c:\\Users\\oalya\\Project-Python\\env\\Lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:03<00:00, 182.53it/s]\n",
      "Detecting Abnormal Objects: 100%|██████████| 200/200 [03:57<00:00,  1.19s/it]\n",
      "Calculating Optical Flow for Video 11: 200it [00:03, 58.33it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 200/200 [00:00<00:00, 2325.10it/s]\n",
      "Detecting Abnormal Objects:  11%|█         | 78/700 [01:33<11:51,  1.14s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects:  11%|█▏        | 79/700 [01:35<14:40,  1.42s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects:  39%|███▉      | 274/700 [04:57<13:03,  1.84s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects:  40%|███▉      | 277/700 [05:06<17:41,  2.51s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [12:57<00:00,  1.11s/it]\n",
      "Calculating Optical Flow for Video 12: 700it [00:15, 45.36it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 709.68it/s]\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [12:41<00:00,  1.09s/it] \n",
      "Calculating Optical Flow for Video 2: 700it [00:41, 16.79it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 1859.56it/s]\n",
      "Detecting Abnormal Objects:  87%|████████▋ | 608/700 [10:34<02:00,  1.31s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [12:12<00:00,  1.05s/it]\n",
      "Calculating Optical Flow for Video 3: 700it [02:04,  5.63it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 1385.86it/s]\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [11:17<00:00,  1.03it/s]\n",
      "Calculating Optical Flow for Video 5: 700it [00:47, 14.71it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 1410.30it/s]\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [13:08<00:00,  1.13s/it]\n",
      "Calculating Optical Flow for Video 7: 700it [00:37, 18.60it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 1619.46it/s]\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [13:58<00:00,  1.20s/it]\n",
      "Calculating Optical Flow for Video 8: 700it [00:35, 19.73it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:00<00:00, 2567.02it/s]\n",
      "Detecting Abnormal Objects:   4%|▎         | 25/700 [00:25<10:15,  1.10it/s]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects:  33%|███▎      | 233/700 [04:37<10:47,  1.39s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects:  41%|████▏     | 290/700 [06:08<07:57,  1.16s/it]WARNING  NMS time limit 0.550s exceeded\n",
      "Detecting Abnormal Objects: 100%|██████████| 700/700 [12:46<00:00,  1.10s/it]\n",
      "Calculating Optical Flow for Video 9: 700it [06:27,  1.81it/s]\n",
      "Extracting Optical Flow Features: 100%|██████████| 700/700 [00:01<00:00, 438.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing videos\n",
    "video_directory = \"./HAJJv2_Dataset/Original_Data/Train/Videos\"\n",
    "\n",
    "# Call the function to process multiple videos\n",
    "process_multiple_videos(video_directory, model, rf_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
